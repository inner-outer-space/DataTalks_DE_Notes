{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f1b6a2",
   "metadata": {},
   "source": [
    "## CONNECTING TO GOOGLE CLOUD STORAGE FROM LOCAL SPARK\n",
    "In the last notebook we created a local session for spark and used the data saved locally. This time we want to pull data from GCS. In order to do that, we need to first upload the parquet files we created before to GCS. \n",
    "\n",
    "gsutil cp -m -r pq/ gs://ny-taxi-data-for-spark/pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f820428",
   "metadata": {},
   "source": [
    "#### GET THE DATA FROM GOOGLE CLOUD\n",
    "you will need to download a connector so that spark will understand how to read this URL \n",
    "You need the .jar file - cloud storage connector for Hadoop3 \n",
    "\n",
    "https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3307b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff7a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a160baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = '/home/lulu/.gcp-keys/dbt-ny-taxi-key.json'\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"lib/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc994c",
   "metadata": {},
   "source": [
    "Create the Spark Context. We previously created this with a builder. We won't use builder here. We will first create a context and then a session. The following says that when you see the file system beginning with gs:// that you need to use this implementation com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem that comes from the jar file we downloaded and the credentials we provided. \n",
    "\n",
    "confirmed that the class was in the version of the jar file I downloaded \n",
    "jar tf gcs-connector-hadoop3-latest.jar | grep GoogleHadoopFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a478ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/14 17:56:08 WARN Utils: Your hostname, pepper resolves to a loopback address: 127.0.1.1; using 192.168.178.64 instead (on interface wlp2s0)\n",
      "24/02/14 17:56:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/02/14 17:56:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/14 17:56:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e703c93",
   "metadata": {},
   "source": [
    "We have a configuration and context. Now we need to create a session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3529b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "390bf152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ny-taxi-data-for-spark/pq/green/2020/\n",
      "gs://ny-taxi-data-for-spark/pq/green/2021/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://ny-taxi-data-for-spark/pq/green/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee1eb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('gs://ny-taxi-data-for-spark/pq/green/*/*')\n",
    "#df_green = spark.read.parquet('gs://mage-zoomcamp-lulu-eu/green/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e767b",
   "metadata": {},
   "source": [
    "I had auth issues and had to add storage object permissions in GCP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca5ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2020-01-12 18:15:04|  2020-01-12 18:19:52|                 N|         1|          41|          41|              1|         0.78|        5.5|  0.0|    0.5|      1.58|         0.0|     NULL|                  0.3|        7.88|           1|        1|                 0.0|\n",
      "|       2| 2020-01-31 20:24:10|  2020-01-31 20:31:51|                 N|         1|         173|          70|              1|         0.98|        7.0|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         8.3|           2|        1|                 0.0|\n",
      "|       2| 2020-01-07 08:16:53|  2020-01-07 08:41:39|                 N|         1|          74|         236|              1|          2.7|       16.0|  0.0|    0.5|      3.91|         0.0|     NULL|                  0.3|       23.46|           1|        1|                2.75|\n",
      "|       1| 2020-01-15 14:47:15|  2020-01-15 14:54:34|                 N|         1|          25|          66|              1|          0.8|        6.5|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|         7.3|           2|        1|                 0.0|\n",
      "|    NULL| 2020-01-31 10:08:00|  2020-01-31 10:20:00|              NULL|      NULL|         259|          51|           NULL|         2.33|      22.49| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|       25.54|        NULL|     NULL|                NULL|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
