{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I noticed there was a difference in size of the parquet and csv files for the Green Jan 2019 Taxi data used in the 2023 cohort homework 1. So I wanted to check the other files that we are working with.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = {\n",
    "    \"2021_01_yellow\":\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\",\n",
    "    \"2019_01_green\":\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-01.parquet\",\n",
    "    \"2019_09_green\":\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-09.parquet\"   \n",
    "    }\n",
    "csv_files = {\n",
    "    \"2021_01_yellow\":\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\",\n",
    "    \"2019_01_green\":\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-01.csv.gz\",\n",
    "    \"2019_09_green\":\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz\"   \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_dfs = {}\n",
    "csv_dfs = {}\n",
    "\n",
    "# Suppress warnings temporarily\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    for name, url in parquet_files.items():\n",
    "        r = requests.get(url)\n",
    "        #df = pd.read_parquet(BytesIO(r.content))\n",
    "        parquet_dfs[name] = pd.read_parquet(BytesIO(r.content))\n",
    "        print(name, parquet_dfs[name].shape)\n",
    "\n",
    "    for name, url in csv_files.items():\n",
    "        r = requests.get(url)\n",
    "        #df = pd.read_csv(BytesIO(r.content), compression='gzip')\n",
    "        csv_dfs[name] = pd.read_csv(BytesIO(r.content), compression='gzip')\n",
    "        print(name, csv_dfs[name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST TO SEE IF THE DFs ARE EQUAL \n",
    "for name in parquet_dfs.keys():\n",
    "    df_csv = csv_dfs[name].reindex(sorted(csv_dfs[name].columns), axis=1)\n",
    "    df_parquet = parquet_dfs[name].reindex(sorted(parquet_dfs[name].columns), axis=1)\n",
    "    are_equal = df_csv.equals(df_parquet)\n",
    "    df_csv_means = df_csv.describe().loc['mean']\n",
    "    df_parquet_means = df_parquet.describe().loc['mean']\n",
    "    are_equal_means = df_csv_means.equals(df_parquet_means)\n",
    "    print(\"DF Comp\", name, are_equal)\n",
    "    print(\"Column Means Comp\", name, are_equal_means)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DOESNT WORK YET\n",
    "    # Extract unique column headers\n",
    "    # Extract unique column headers from both DataFrames\n",
    "    unique_columns_csv = set(df_csv.columns.tolist())\n",
    "    unique_columns_parquet = set(df_parquet.columns.tolist())\n",
    "\n",
    "    # Combine unique columns from both DataFrames\n",
    "    unique_columns_combined = unique_columns_csv.union(unique_columns_parquet)\n",
    "\n",
    "    # Convert the combined unique columns to a list\n",
    "    unique_columns_combined = list(unique_columns_combined)\n",
    "\n",
    "    # Create an empty DataFrame with the unique column headers as columns\n",
    "    combined_df = pd.DataFrame(columns=unique_columns_combined)\n",
    "\n",
    "    # Populate the combined DataFrame with values from df_csv\n",
    "    for column in df_csv.columns:\n",
    "        combined_df[column] = df_csv_means[column]\n",
    "\n",
    "    # Populate the combined DataFrame with values from df_parquet\n",
    "    for column in df_parquet.columns:\n",
    "        combined_df[column] = df_parquet_means[column]\n",
    "\n",
    "    # Now combined_df contains all unique column headers and their respective values\n",
    "    print(combined_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de_zoom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
